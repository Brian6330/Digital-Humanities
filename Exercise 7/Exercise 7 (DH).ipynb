{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 - Vector space model P2\n",
    "\n",
    "First name: Brian\n",
    "<br>\n",
    "Last name: Schweigler\n",
    "<br>\n",
    "Matriculation number: 16-102-071\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1: For each genre, generate a “profile” in the form of a single vector representing the entire set of plays corresponding to this genre. Build such a profile for each of the three genres (Comedy, Tragedy and Tragicomedy).\n",
    "\n",
    "Can take this from the solutions of last series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498 498 498 498 498\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import lxml.etree\n",
    "import os\n",
    "from scipy import stats\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "import collections\n",
    "\n",
    "subgenres = ('Comédie', 'Tragédie', 'Tragi-comédie') # Three subgenres, Comedy, Tragedy or Tragicomedy\n",
    "plays, titles, genres, authors, dates = [], [], [],[], [] # Initialize empty lists for recursion\n",
    "\n",
    "for file in os.scandir('theatre-classique'): # For loop through files\n",
    "    if not file.name.endswith('.xml'): # If the file is not an .xml\n",
    "       continue # Do nothing and go to next iteration\n",
    "    tree   = lxml.etree.parse(file.path) # Parse file\n",
    "    genre  = tree.find('//genre') # Find genre\n",
    "    title  = tree.find('//title') # Find title\n",
    "    author = tree.find('//author') # Find author\n",
    "    date   = tree.find('//date') # Find date\n",
    "    if genre is not None and genre.text in subgenres: # Parse only plays for which we know the genre\n",
    "        lines = []\n",
    "        for line in tree.xpath('//l|//p'): # The actual play text in these files is matched by tags p and l\n",
    "            lines.append(' '.join(line.itertext()))\n",
    "        text = '\\n'.join(lines) # Generate the play\n",
    "        plays.append(text) # Append the play\n",
    "        genres.append(genre.text) # Append the genre\n",
    "        titles.append(title.text) # Append the title\n",
    "        if author is not None: # There can be missing authors to handle\n",
    "            authors.append(author.text)\n",
    "        else:\n",
    "            authors.append('') # We put an empty string\n",
    "        if date is not None: # There can be missing dates to handle\n",
    "            dates.append(date.text)\n",
    "        else:\n",
    "            dates.append('') # We put an empty string\n",
    "\n",
    "print (len(plays), len(genres), len(titles), len(authors), len(dates)) # Should be same size!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # RegExp library\n",
    "import nltk # Python library for NLP\n",
    "\n",
    "punctuation_rule = re.compile(r'[^\\w\\s]+$') # RegExp that matches punctuations that occur one or more times\n",
    "\n",
    "def is_punctuation(string):\n",
    "    \"\"\"\n",
    "    Check if STRING is a punctuation marker or a sequence of\n",
    "    punctuation markers.\n",
    "    \"\"\"\n",
    "    return punctuation_rule.match(string) is not None # Return punctuation if present\n",
    "\n",
    "def preprocess_text(text, language='french', lowercase=True):\n",
    "    \"\"\"\n",
    "    Preprocess input text. All to lowercase, sub some common\n",
    "    French language patterns.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower() # All words to lowercase\n",
    "\n",
    "    if language == 'french': # Preprocess common patterns for French language\n",
    "        text = re.sub(\"-\", \" \", text)\n",
    "        text = re.sub(\"l'\", \"le \", text)\n",
    "        text = re.sub(\"d'\", \"de \", text)\n",
    "        text = re.sub(\"c'\", \"ce \", text)\n",
    "        text = re.sub(\"j'\", \"je \", text)\n",
    "        text = re.sub(\"m'\", \"me \", text)\n",
    "        text = re.sub(\"qu'\", \"que \", text)\n",
    "        text = re.sub(\"'\", \" ' \", text)\n",
    "        text = re.sub(\"quelqu'\", \"quelque \", text)\n",
    "        text = re.sub(\"aujourd'hui\", \"aujourdhui\", text)\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language) # Tokenize specifying the language\n",
    "    tokens = [token for token in tokens if not is_punctuation(token)] # Exclude punctuations\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally tokenize our lines as it follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays_token = [preprocess_text(play, 'french') for play in plays] # Tokenize every play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plays_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These computation let us preprocess the original text and generate a tokenized corpus. Now we can extract from it a vocabulary with a minimum and maximum frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62967"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections # Library to simplify tallies\n",
    "\n",
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"\n",
    "    Extract vocabulary from input tokenized text.\n",
    "    Min frequency count of a vocabulary item is set to 1 and max to infinite.\n",
    "    \"\"\"\n",
    "    vocab = collections.Counter() # Create a container object for rapid tallies\n",
    "    for document in tokenized_corpus:\n",
    "        vocab.update(document) # Update for each play\n",
    "    vocab = {word for word, count in vocab.items()\n",
    "             if min_count <= count <= max_count} # Append only if the word frequency is between the boundaries\n",
    "    return sorted(vocab) # Return a list alphabetically ordered of unique words in the corpus\n",
    "\n",
    "vocabulary = extract_vocabulary(plays_token) # Build the vocabulary\n",
    "len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to represent each play with a vector of term frequencies, we create a document-term matrix (DTM).\n",
    "In this representation, each row is a play in our corpus and each column a unique word with the respective frequency count ($\\textit{tf}$).\n",
    "The words are ordered as they appear in the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix with 498 documents and 62967 words.\n"
     ]
    }
   ],
   "source": [
    "def corpus2dtm(tokenized_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Custom function to transform a tokenized corpus into a document-term matrix.\n",
    "    \"\"\"\n",
    "    dtm = []\n",
    "    for document in tokenized_corpus: # For each play\n",
    "        document_counts = collections.Counter(document) # Get counters\n",
    "        row = [document_counts[word] for word in vocab] # Count tf for each word in the vocabulary\n",
    "        dtm.append(row) # Append row\n",
    "    return dtm\n",
    "\n",
    "\n",
    "document_term_matrix = np.array(corpus2dtm(plays_token, vocabulary)) # Build the DTM\n",
    "print(f'Document-term matrix with {document_term_matrix.shape[0]} documents and {document_term_matrix.shape[1]} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62967,) (62967,) (62967,)\n"
     ]
    }
   ],
   "source": [
    "genres_as_list = np.array(genres) # List to array, for computations\n",
    "\n",
    "tragedy_profile = document_term_matrix[genres_as_list == 'Tragédie'].mean(axis=0)\n",
    "comedy_profile = document_term_matrix[genres_as_list == 'Comédie'].mean(axis=0)\n",
    "tragicomedy_profile = document_term_matrix[genres_as_list == 'Tragi-comédie'].mean(axis=0)\n",
    "\n",
    "print(tragedy_profile.shape, comedy_profile.shape, tragicomedy_profile.shape) # Single vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2: Which are the three plays for each text genre (or group) that are the “closest” to the profile?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tragédies - comédies:       447.90\n",
      "tragédies - tragi-comédies: 301.75\n",
      " comédies - tragi-comédies: 656.17\n",
      "Min distance is: 447.89800112267636 which is at index: 0 for tragedies.\n",
      "Min distance is: 929.8192306271798 which is at index: 4126 for comedies.\n",
      "Min distance is: 1533.573447893221 which is at index: 738 for tragi-comedie.\n"
     ]
    }
   ],
   "source": [
    "# Compute the Euclidan distance\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "tc = euclidean_distance(tragedy_profile, comedy_profile)\n",
    "print(f'tragédies - comédies:       {tc:.2f}')\n",
    "ttc = euclidean_distance(tragedy_profile, tragicomedy_profile)\n",
    "print(f'tragédies - tragi-comédies: {ttc:.2f}')\n",
    "ctc = euclidean_distance(comedy_profile, tragicomedy_profile)\n",
    "print(f' comédies - tragi-comédies: {ctc:.2f}')\n",
    "\n",
    "trag_distances = []\n",
    "for row in document_term_matrix[genres_as_list == 'Tragédie'].mean(axis=0):\n",
    "    trag_distances.append(tc)\n",
    "    tc = euclidean_distance(tragedy_profile, row)\n",
    "print(\"Min distance is: \" + str(min(trag_distances)) + \" which is at index: \" +\n",
    "      str(trag_distances.index(min(trag_distances))) + \" for tragedies.\")\n",
    "\n",
    "com_distances = []\n",
    "for row in document_term_matrix[genres_as_list == 'Comédie'].mean(axis=0):\n",
    "    ttc = euclidean_distance(comedy_profile, row)\n",
    "    com_distances.append(ttc)\n",
    "print(\"Min distance is: \" + str(min(com_distances)) + \" which is at index: \" +\n",
    "      str(com_distances.index(min(com_distances))) + \" for comedies.\")\n",
    "\n",
    "trc_distances = []\n",
    "for row in document_term_matrix[genres_as_list == 'Tragi-comédie'].mean(axis=0):\n",
    "    ctc = euclidean_distance(tragicomedy_profile, row)\n",
    "    trc_distances.append(ctc)\n",
    "print(\"Min distance is: \" + str(min(trc_distances)) + \" which is at index: \" +\n",
    "      str(trc_distances.index(min(trc_distances))) + \" for tragi-comedie.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3: Usually, we generate a profile by averaging over all term frequencies of plays belonging to a certain group. Do you know another way to generate a profile from a set of documents (or vectors)?\n",
    "\n",
    "As seen in the lecture, we could use the keyness of the terms or the rTF (relative term frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Do you think that the profile must include all words appearing at least once in a play of the group? If no, how can we select a subset of the terms that must appear in a profile? Justify your choice..\n",
    "\n",
    "We should definitely only have a subset of words; I vouch for a.) stemming the words to their base forms and b.) removing stop-words.\n",
    "The former gives clearer profiels where certain terms can occur more often, while the latter removes the \"noise\".\n",
    "\n",
    "All plays use the most popular words, but those are not what characterize a play or a document.\n",
    "Those, by combining those two, clearer profiles can be made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
